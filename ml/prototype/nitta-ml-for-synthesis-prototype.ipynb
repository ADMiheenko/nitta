{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NITTA ML for Synthesis Prototype\n",
    "\n",
    "## 1. Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession\n",
    "from cached_property import cached_property\n",
    "from cachetools import cached\n",
    "from dataclasses_json import dataclass_json, LetterCase\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NITTA_PORT = 53829\n",
    "NITTA_BASEURL = f\"http://localhost:{NITTA_PORT}\"\n",
    "NITTA_ROOT_DIR = r'../..'\n",
    "# may needed to be updated\n",
    "NITTA_EXE_PATH = \"stack exec nitta -- \" # NITTA_ROOT_DIR + r\"\\.stack-work\\dist\\29cc6475\\build\\NITTA\\nitta.exe\"\n",
    "METRICS_WEIGHTS = pd.Series(dict(duration=-1, depth=-0.1))\n",
    "LAMBDA = 0.6\n",
    "WAIT_NITTA_DELAY = 0.5\n",
    "pd.set_option('display.max_colwidth', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_node_method(wrapped):\n",
    "    return cached({}, key=lambda self, *args: hash(self.sid))(wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debounce(s):\n",
    "    \"\"\"Decorator ensures function that can only be called once every `s` seconds.\n",
    "    \"\"\"\n",
    "    def decorate(f):\n",
    "        t = None\n",
    "        n = 0\n",
    "\n",
    "        def wrapped(*args, **kwargs):\n",
    "            nonlocal t, n\n",
    "            t_ = time.time()\n",
    "            if t is None or t_ - t >= s:\n",
    "                if n > 0: print(f\"-- skipped {n} calls\")\n",
    "                result = f(*args, **kwargs)\n",
    "                t = time.time()\n",
    "                n = 0\n",
    "                return result\n",
    "            else:\n",
    "                n += 1\n",
    "        return wrapped\n",
    "    return decorate\n",
    "\n",
    "@debounce(1)\n",
    "def log_debug(*args):\n",
    "    print(\"--\", datetime.datetime.now().strftime(\"%T\"), *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nitta_dataclass_params = dict(letter_case=LetterCase.CAMEL)\n",
    "\n",
    "\n",
    "@dataclass_json(**nitta_dataclass_params)\n",
    "@dataclass\n",
    "class NittaNodeDecision:\n",
    "    tag: str\n",
    "    \n",
    "@dataclass_json(**nitta_dataclass_params)\n",
    "@dataclass\n",
    "class NittaNode: \n",
    "    score: Optional[int]\n",
    "    is_terminal: bool\n",
    "    is_finish: bool\n",
    "    parameters: Any # NittaNodeParameters\n",
    "    decision: NittaNodeDecision\n",
    "    duration: Optional[int]\n",
    "    sid: str\n",
    "        \n",
    "    children: Optional[List['NittaNode']] = field(default=None, repr=False)\n",
    "    parent: Optional['NittaNode'] = field(default=None, repr=False)\n",
    "        \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.is_terminal\n",
    "        \n",
    "    @cached_property\n",
    "    def subtree_size(self):\n",
    "        assert self.children is not None\n",
    "        return sum(child.subtree_size for child in self.children) + 1\n",
    "        \n",
    "    @cached_property\n",
    "    def depth(self) -> int:\n",
    "        return self.sid.count('-') if self.sid != '-' else 0\n",
    "       \n",
    "    @cached_property\n",
    "    def subtree_leafs_metrics(self) -> pd.DataFrame:\n",
    "        if self.is_leaf:\n",
    "            if not self.is_finish:\n",
    "                return pd.DataFrame()\n",
    "            return pd.DataFrame(dict(duration=[self.duration], depth=[self.depth]))\n",
    "        else:\n",
    "            return pd.concat([child.subtree_leafs_metrics for child in self.children])\n",
    "        \n",
    "    @cached_node_method\n",
    "    def get_subtree_leafs_labels(self, metrics_distrib: pd.DataFrame) -> pd.Series:\n",
    "        if self.is_leaf:\n",
    "            return pd.Series([self.compute_label(metrics_distrib)])\n",
    "        else:\n",
    "            return pd.concat([child.get_subtree_leafs_labels(metrics_distrib) for child in self.children])\n",
    "    \n",
    "    @cached_node_method\n",
    "    def compute_label(self, metrics_distrib: pd.DataFrame) -> float:\n",
    "        if self.is_leaf: \n",
    "            if not self.is_finish:\n",
    "                # unsuccessful synthesis, very low artificial label\n",
    "                return -3\n",
    "            \n",
    "            metrics = self.subtree_leafs_metrics.iloc[0]\n",
    "            normalized_metrics = (metrics - metrics_distrib.mean()) / metrics_distrib.std()\n",
    "            return normalized_metrics.dot(METRICS_WEIGHTS)\n",
    "        \n",
    "        subtree_labels = self.get_subtree_leafs_labels(metrics_distrib)\n",
    "        return LAMBDA * subtree_labels.max() + (1 - LAMBDA) * subtree_labels.mean()\n",
    "    \n",
    "    @cached_property\n",
    "    def alternative_siblings(self) -> dict:\n",
    "        bindings, refactorings, dataflows = 0, 0, 0\n",
    "        \n",
    "        if self.parent:\n",
    "            for sibling in self.parent.children:\n",
    "                if sibling.sid == self.sid:\n",
    "                    continue\n",
    "                target = None\n",
    "                if sibling.decision.tag == \"BindDecisionView\":\n",
    "                    bindings += 1\n",
    "                elif sibling.decision.tag == \"DataflowDecisionView\":\n",
    "                    dataflows += 1\n",
    "                else:\n",
    "                    refactorings += 1\n",
    "            \n",
    "        return dict(alt_bindings=bindings, \n",
    "                    alt_refactorings=refactorings, \n",
    "                    alt_dataflows=dataflows)\n",
    "            \n",
    "    async def retrieve_subforest(self, session, levels_left=None):\n",
    "        self.children = []\n",
    "        if self.is_leaf or levels_left == -1:\n",
    "            return\n",
    "\n",
    "        async with session.get(NITTA_BASEURL + f\"/node/{self.sid}/subForest\") as resp:\n",
    "            children_raw = await resp.json()\n",
    "            \n",
    "        log_debug(f\"{len(children_raw)} children from {self.sid}\")\n",
    "\n",
    "        for child_raw in children_raw:\n",
    "            child = NittaNode.from_dict(child_raw)\n",
    "            child.parent = self\n",
    "            self.children.append(child)\n",
    "\n",
    "        levels_left_for_child = None if levels_left is None else levels_left - 1\n",
    "        await asyncio.gather(\n",
    "            *[child.retrieve_subforest(session, levels_left_for_child) for child in self.children]\n",
    "        )\n",
    "        \n",
    "                    \n",
    "async def retrieve_whole_nitta_tree(max_depth=None) -> NittaNode:\n",
    "    start_time = time.perf_counter()\n",
    "    async with ClientSession() as session:\n",
    "        async with session.get(NITTA_BASEURL + f\"/node/-\") as resp:\n",
    "            root_raw = await resp.json()\n",
    "        root = NittaNode.from_dict(root_raw)\n",
    "        await root.retrieve_subforest(session, max_depth)\n",
    "    \n",
    "    print(f\"Finished tree retrieval in {time.perf_counter() - start_time:.2f} s\")\n",
    "    return root\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Test retrieve_whole_nitta_tree\")\n",
    "\n",
    "example = \"examples/counter.lua\"\n",
    "nitta_tree = None\n",
    "with subprocess.Popen(f\"{NITTA_EXE_PATH} -p={NITTA_PORT} {example}\", \n",
    "                      cwd=NITTA_ROOT_DIR, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n",
    "                     ) as proc:\n",
    "    try:\n",
    "        time.sleep(WAIT_NITTA_DELAY)\n",
    "        print(proc.stdout.read1().decode())\n",
    "        nitta_tree = await retrieve_whole_nitta_tree()\n",
    "        print(f\"Nodes: {nitta_tree.subtree_size}\")\n",
    "    finally:\n",
    "        proc.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"treedump.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(nitta_tree, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"treedump.pickle\", \"rb\") as f:\n",
    "#     nitta_tree = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuple(nitta_tree.subtree_leafs_metrics.iloc[0].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tree size: {nitta_tree.subtree_size} nodes\")\n",
    "_ = nitta_tree.subtree_leafs_metrics.hist(figsize=(9, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nitta_tree.subtree_leafs_metrics.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nitta_tree.compute_label(nitta_tree.subtree_leafs_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nitta_tree.get_subtree_leafs_labels(nitta_tree.subtree_leafs_metrics).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_labels(tree, metrics_distrib):\n",
    "    labels = []\n",
    "    def nodejob(node):\n",
    "        labels.append({\"label\": node.compute_label(metrics_distrib), \n",
    "                       \"depth\": node.depth, \n",
    "                       \"duration\": node.duration})\n",
    "        for child in node.children:\n",
    "            nodejob(child)\n",
    "            \n",
    "    nodejob(tree)\n",
    "    return pd.DataFrame(labels)\n",
    "    \n",
    "ds = nitta_tree.subtree_leafs_metrics\n",
    "lbls = collect_all_labels(nitta_tree, ds)\n",
    "_ = lbls.label.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best(node, metrics_distrib):\n",
    "    if node.is_leaf:\n",
    "        return node\n",
    "\n",
    "    best_child = max([(child.compute_label(metrics_distrib), child) for child in node.children], key=lambda v: v[0])[1]\n",
    "    return select_best(best_child, metrics_distrib)\n",
    "\n",
    "best = select_best(nitta_tree, ds)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = nitta_tree.subtree_leafs_metrics\n",
    "print(f\"depths with best node's duration ({best.duration}):\")\n",
    "display(lfs[lfs.duration == best.duration].depth.value_counts())\n",
    "print(f\"best node's depth: {best.depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NITTA synthesis tree to CSV dataset implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_params_dict(node: NittaNode) -> dict:\n",
    "    if node.decision.tag in [\"BindDecisionView\", \"DataflowDecisionView\"]:\n",
    "        result = node.parameters.copy()\n",
    "        if node.decision.tag == \"DataflowDecisionView\":\n",
    "            result[\"pNotTransferableInputs\"] = sum(result[\"pNotTransferableInputs\"])\n",
    "        return result\n",
    "    elif node.decision.tag == \"RootView\":\n",
    "        return {}\n",
    "    else:\n",
    "        # refactorings\n",
    "        return {\"pRefactoringType\": node.decision.tag}\n",
    "\n",
    "\n",
    "def assemble_tree_dataframe(example: str, node: NittaNode, metrics_distrib=None, include_label=True,\n",
    "                            levels_left=None) -> pd.DataFrame:\n",
    "    if include_label and metrics_distrib is None:\n",
    "        metrics_distrib = node.subtree_leafs_metrics\n",
    "\n",
    "    self_df = pd.DataFrame(dict(\n",
    "        example=example,\n",
    "        sid=node.sid,\n",
    "        tag=node.decision.tag,\n",
    "        old_score=node.score,\n",
    "        is_leaf=node.is_leaf,\n",
    "        **node.alternative_siblings,\n",
    "        **_extract_params_dict(node),\n",
    "    ), index=[0])\n",
    "    if include_label:\n",
    "        self_df[\"label\"] = node.compute_label(metrics_distrib)\n",
    "\n",
    "    levels_left_for_child = None if levels_left is None else levels_left - 1\n",
    "    if node.is_leaf or levels_left == -1:\n",
    "        return self_df\n",
    "    else:\n",
    "        result = [assemble_tree_dataframe(example, child, metrics_distrib, include_label, levels_left_for_child)\n",
    "                  for child in node.children]\n",
    "        if node.sid != \"-\":\n",
    "            result.insert(0, self_df)\n",
    "        return pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = assemble_tree_dataframe(\"spi3\", nitta_tree, levels_left=0)\n",
    "display(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = assemble_tree_dataframe(\"spi3\", nitta_tree, levels_left=2)\n",
    "display(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[(tdf.sid == \"-0-0-0\") | (tdf.sid == \"-0-0-1\") | (tdf.sid == \"-0-0-2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"data\")\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/2470971/fast-way-to-test-if-a-port-is-in-use-using-python\n",
    "def is_port_in_use(port):\n",
    "    import socket\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        return s.connect_ex(('localhost', port)) == 0\n",
    "\n",
    "async def process_example(example: str) -> pd.DataFrame:\n",
    "    if is_port_in_use(NITTA_PORT):\n",
    "        raise RuntimeError(f\"Port {NITTA_PORT} is already in use, shutdown NITTA server if that's running.\")\n",
    "        \n",
    "    example_name = os.path.basename(example)\n",
    "    df = None\n",
    "\n",
    "    print(f\"Processing example {example!r}\")\n",
    "    with subprocess.Popen(f\"{NITTA_EXE_PATH} -p={NITTA_PORT} {example}\", cwd=NITTA_ROOT_DIR,\n",
    "                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True) as proc:\n",
    "        try:\n",
    "            print(f\"NITTA is running.\")\n",
    "            time.sleep(WAIT_NITTA_DELAY)\n",
    "            print(f\"Retrieving tree...\")\n",
    "\n",
    "            tree = await retrieve_whole_nitta_tree()\n",
    "            with open(DATA_ROOT / f\"{example_name}.pickle\", \"wb\") as f:\n",
    "                pickle.dump(tree, f)\n",
    "\n",
    "            print(f\"Nodes: {tree.subtree_size}. Building dataframe...\")\n",
    "            df = assemble_tree_dataframe(example_name, tree).reset_index(drop=True)\n",
    "\n",
    "            print(f\"Data's ready, {len(df)} rows\")\n",
    "\n",
    "            target_filepath = DATA_ROOT / f\"{example_name}.csv\"\n",
    "            print(f\"Saving to {target_filepath}\")\n",
    "            df.to_csv(target_filepath, index=False)\n",
    "        finally:\n",
    "            proc.kill()\n",
    "            print(f\"NITTA is dead\")\n",
    "    print(\"DONE\")\n",
    "    return df\n",
    "\n",
    "print(\"Available examples:\")\n",
    "examples = list(map(os.path.abspath, glob(f'{NITTA_ROOT_DIR}/examples/*.lua')))\n",
    "pd.Series(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = await process_example(examples[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import missingno\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"data\")\n",
    "\n",
    "data_csvs = glob(str(DATA_ROOT / \"*.csv\"))\n",
    "print(\"Available CSVs:\")\n",
    "data_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(d) for d in data_csvs]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling to cope with imbalanced data\n",
    "# dfu = pd.concat([\n",
    "#     df,\n",
    "#     pd.concat([df[df.example == \"counter.lua\"]]*(8 - 1)),\n",
    "#     pd.concat([df[df.example == \"fibonacci.lua\"]]*(50 - 1)),\n",
    "#     pd.concat([df[df.example == \"spi2.lua\"]]*(100 - 1))\n",
    "# ])\n",
    "dfu = df\n",
    "dfu.example.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def map_bool(c):\n",
    "        return c.apply(lambda v: 1 if v is True else (0 if v is False else v))\n",
    "    \n",
    "    def map_categorical(df, c, options=None):\n",
    "        return pd.concat([df.drop([c.name], axis=1), pd.get_dummies(c, prefix=c.name, columns=options)], axis=1)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.is_leaf = map_bool(df.is_leaf)\n",
    "    df.pCritical = map_bool(df.pCritical)\n",
    "    df.pPossibleDeadlock = map_bool(df.pPossibleDeadlock)\n",
    "    df.pRestrictedTime = map_bool(df.pRestrictedTime)\n",
    "    df = map_categorical(df, df.tag, ['tag_BindDecisionView','tag_BreakLoopView','tag_ConstantFoldingView','tag_DataflowDecisionView','tag_OptimizeAccumView','tag_ResolveDeadlockView'])\n",
    "    df = df.drop([\"pWave\", \"example\", \"sid\", \"old_score\", \"is_leaf\", \"pRefactoringType\"], axis=\"columns\")\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "pdf = preprocess_df(dfu)\n",
    "pdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_COLUMNS = pdf.columns.tolist()\n",
    "display(FINAL_COLUMNS)\n",
    "METRICS_COLUMNS = [cn for cn in FINAL_COLUMNS if cn.startswith(\"p\")] + [\"pRefactoringType\", \"pWave\"]\n",
    "display(METRICS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(pdf.sort_values([\"pOutputNumber\", \"pWaitTime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple\n",
    "\n",
    "TARGET_COLUMNS = [\"label\"]\n",
    "def create_datasets(df) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "    # create training and evaluation datasets\n",
    "    train_df, test_df = train_test_split(df.sample(frac=1), test_size=0.2)\n",
    "\n",
    "    N = len(df)\n",
    "    print(f\"N:\\t{N}\")\n",
    "    print(f\"Train:\\t{len(train_df)}, {len(train_df) / N * 100:.0f}%\")\n",
    "    print(f\"Test:\\t{len(test_df)}, {len(test_df) / N * 100:.0f}%\")\n",
    "    \n",
    "    def df_to_dataset(df, shuffle=True, batch_size=16, repeat=False, print_cols=False):\n",
    "        df = df.copy()\n",
    "\n",
    "        # split df into features and labels\n",
    "        targets = df[TARGET_COLUMNS].copy()\n",
    "        df.drop(TARGET_COLUMNS, axis=1, inplace=True)\n",
    "        features = df\n",
    "        if print_cols:\n",
    "            print(f\"Feature columns: {features.columns.values.tolist()}\")\n",
    "\n",
    "        ds = tf.data.Dataset.from_tensor_slices((features.values, targets.values))\n",
    "        ds = ds.shuffle(buffer_size=10000) if shuffle else ds\n",
    "        ds = ds.batch(batch_size) if batch_size else ds\n",
    "        ds = ds.repeat() if repeat else ds\n",
    "        return ds\n",
    "\n",
    "    train_ds = df_to_dataset(train_df, batch_size=16, repeat=True, print_cols=True)\n",
    "    test_ds = df_to_dataset(test_df)\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = create_datasets(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def create_model(df, **kwargs) -> tf.keras.Model:\n",
    "    feature_columns = [\n",
    "        tf.feature_column.numeric_column(c)\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(20,)),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "        layers.Dense(128, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "        layers.Dense(64, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),  # Optimizer\n",
    "        # Loss function to minimize\n",
    "        loss=\"mse\",\n",
    "        # List of metrics to monitor\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hist_df = pd.DataFrame()\n",
    "model = create_model(pdf)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=60,\n",
    "    steps_per_epoch=2250,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "hist_df = pd.DataFrame(raw_history.history)\n",
    "total_hist_df = pd.concat([total_hist_df, hist_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df[[\"loss\", \"val_loss\"]].plot()\n",
    "plt.grid()\n",
    "hist_df[[\"mae\", \"val_mae\"]].plot()\n",
    "plt.grid()\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_hist_df[[\"loss\", \"val_loss\"]].plot()\n",
    "plt.grid()\n",
    "total_hist_df[[\"mae\", \"val_mae\"]].plot()\n",
    "plt.grid()\n",
    "total_hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importance by weights absolute value:\")\n",
    "pd.DataFrame(zip(np.abs(model.weights[0].numpy()).sum(axis=1), set(pdf.columns) - set(TARGET_COLUMNS))) \\\n",
    "    .set_index(1) \\\n",
    "    .sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_ROOT = Path(\"models\")\n",
    "MODELS_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "mname = \"model_N\"\n",
    "model.save(MODELS_ROOT / mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally save total_hist_df for reference\n",
    "# with open(MODELS_ROOT / f\"{mname}_total_hist.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(total_hist_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = Path(\"models\")\n",
    "\n",
    "# model = tf.keras.models.load_model(MODELS_ROOT / \"example_model_1\")\n",
    "model = tf.keras.models.load_model(MODELS_ROOT / mname)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_evaluator(node: NittaNode):\n",
    "    final_columns = ['alt_bindings', 'alt_refactorings', 'alt_dataflows', 'pOutputNumber', 'pAlternative', 'pAllowDataFlow', 'pCritical', 'pPercentOfBindedInputs', 'pPossibleDeadlock', 'pNumberOfBindedFunctions', 'pRestless', 'pNotTransferableInputs', 'pRestrictedTime', 'pWaitTime', 'tag_BindDecisionView', 'tag_BreakLoopView', 'tag_ConstantFoldingView', 'tag_DataflowDecisionView', 'tag_OptimizeAccumView', 'tag_ResolveDeadlockView']\n",
    "    metrics_columns = [cn for cn in final_columns if cn.startswith(\"p\")] + [\"pRefactoringType\", \"pWave\"]\n",
    "    \n",
    "    node_df = assemble_tree_dataframe(\"\", node, include_label=False, levels_left=-1)\n",
    "    filled_metrics_df = pd.concat([pd.DataFrame(columns=metrics_columns), node_df])\n",
    "    preprocessed_df = preprocess_df(filled_metrics_df)\n",
    "    right_final_columns_df = pd.concat([pd.DataFrame(columns=final_columns), preprocessed_df])[final_columns]\n",
    "    ohe_flags_zero_filled_df = right_final_columns_df.fillna(0)\n",
    "    final_df = ohe_flags_zero_filled_df\n",
    "    \n",
    "    return model.predict(final_df.values)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ServerDisconnectedError\n",
    "from collections import defaultdict\n",
    "\n",
    "def old_evaluator(node: NittaNode):\n",
    "    return node.score\n",
    "\n",
    "counters = defaultdict(lambda: 0)\n",
    "def reset_counters():\n",
    "    global counters\n",
    "    counters = defaultdict(lambda: 0)\n",
    "\n",
    "async def select_best_by_evaluator(session, evaluator, node, children_limit=None):\n",
    "    counters[evaluator.__name__] += 1\n",
    "    \n",
    "    if node.is_leaf:\n",
    "        if not node.is_finish:\n",
    "            return None\n",
    "            \n",
    "        return node\n",
    "\n",
    "    try:\n",
    "        await node.retrieve_subforest(session, 0)\n",
    "    except ServerDisconnectedError:\n",
    "#         print(f\"Invalid node with NITTA exception: {node}\")\n",
    "        return None\n",
    "    \n",
    "    children = [(evaluator(child), child) for child in node.children]\n",
    "    children.sort(key=lambda v: v[0], reverse=True)\n",
    "#     print(f\"children: {[d[0] for d in children]}\")\n",
    "    if children_limit:\n",
    "        children = children[:children_limit]\n",
    "    \n",
    "    while children:\n",
    "        next_best_child = children.pop(0)[1]\n",
    "#         print(f\"next best: {next_best_child}\")\n",
    "        result = await select_best_by_evaluator(session, evaluator, next_best_child, children_limit)\n",
    "        if result is not None:\n",
    "            return result         \n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_counters()\n",
    "\n",
    "example = \"examples/counter.lua\"\n",
    "nitta_tree = None\n",
    "with subprocess.Popen(f\"{NITTA_EXE_PATH} -p={NITTA_PORT} {example}\", \n",
    "                      cwd=NITTA_ROOT_DIR, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n",
    "                     ) as proc:\n",
    "    try:\n",
    "        time.sleep(WAIT_NITTA_DELAY)\n",
    "        print(proc.stdout.read1().decode())\n",
    "        nitta_tree = await retrieve_whole_nitta_tree()\n",
    "    finally:\n",
    "        proc.kill()\n",
    "        \n",
    "new_evaluator(nitta_tree.children[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reset_counters()\n",
    "\n",
    "example = \"examples/counter.lua\"\n",
    "nitta_tree = None\n",
    "with subprocess.Popen(f\"{NITTA_EXE_PATH} -p={NITTA_PORT} {example}\", \n",
    "                      cwd=NITTA_ROOT_DIR, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n",
    "                     ) as proc:\n",
    "    try:\n",
    "        time.sleep(WAIT_NITTA_DELAY)\n",
    "        print(proc.stdout.read1().decode())\n",
    "\n",
    "        root = await retrieve_whole_nitta_tree()\n",
    "\n",
    "        async with ClientSession() as session:\n",
    "            best_new = await select_best_by_evaluator(session, new_evaluator, root, 2)\n",
    "            print(\"NEW DONE\", best_new)\n",
    "            best_old = await select_best_by_evaluator(session, old_evaluator, root, 2)\n",
    "            print(\"OLD DONE\", best_old)\n",
    "    finally:\n",
    "        proc.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(best_old)\n",
    "display(best_new)\n",
    "display(pd.DataFrame(dict(duration=[best_old.duration, best_new.duration],\n",
    "                          depth=[best_old.depth, best_new.depth],\n",
    "                          evaluator_calls=[counters[\"old_evaluator\"], counters[\"new_evaluator\"]]), index=[\"old\", \"new\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
